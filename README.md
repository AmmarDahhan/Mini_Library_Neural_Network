# بناء مكتبة شبكات عصبونية مصغرة من الصفر

## مقدمة عن المشروع

هذا المشروع هو تطبيقي لوظيفة جامعية تهدف إلى بناء مكتبة شبكات عصبونية صغيرة من الصفر بالاعتماد الكامل على مكتبة `NumPy`. الهدف الأساسي كان فهم الآليات الجوهرية للشبكات العصبونية، مثل الانتشار الأمامي والخلفي، عن طريق برمجتها يدوياً بدلاً من استخدام أطر عمل جاهزة مثل PyTorch أو TensorFlow.

---

## المفاهيم الأساسية التي تم تطبيقها

- **الطبقات (Layers):** قمت ببناء الطبقات الأساسية:
  - `Affine`: الطبقة متصلة بالكامل (Fully Connected).
  - `ReLU`: دالة التنشيط.
  - `SoftmaxWithLoss`: طبقة مدمجة للناتج النهائي وحساب الخطأ.
- **الانتشار الأمامي (Forward Propagation):** تمرير البيانات عبر الشبكة لتوليد التنبؤات.
- **الانتشار الخلفي (Backpropagation):** حساب التدرجات (Gradients) لجميع الأوزان والانحيازات باستخدام قاعدة السلسلة.
- **المُحسِّنات (Optimizers):** قمت بتطبيق خوارزميتين لتحديث الأوزان:
  - `SGD`: التدرج العشوائي البسيط.
  - `Adam`: خوارزمية أكثر تطوراً وفعالية.
- **حلقة التدريب (Training Loop):** سكربت كامل لتدريب الشبكة على مجموعة بيانات MNIST، مع تقسيم البيانات إلى دفعات (mini-batches) وحساب الدقة.

---

## هيكلية المشروع

المشروع مقسم إلى 4 ملفات رئيسية، كل منها له وظيفة محددة:

- **`layers.py`**: يحتوي على "قطع الليغو" للمشروع. كل كلاس في هذا الملف يمثل طبقة (مثل `Relu` أو `Affine`) ويحتوي على دالتي `forward` و `backward` الأساسيتين.

- **`network.py`**: هنا قمت بتعريف كلاس `TwoLayerNet` الذي يجمع الطبقات من `layers.py` لبناء الهيكل الكامل للشبكة العصبونية.

- **`optimizer.py`**: يحتوي على "محركات" التعلم. قمت بتطبيق `SGD` و `Adam` هنا، وهي المسؤولة عن تحديث أوزان الشبكة بناءً على التدرجات المحسوبة.

- **`train.py`**: هذا هو السكربت الرئيسي الذي يقوم بتشغيل كل شيء. يقوم بتحميل بيانات MNIST، وإنشاء كائن من الشبكة والمُحسِّن، ثم يبدأ حلقة التدريب والتقييم.

---

## كيفية تشغيل المشروع وتجربة النماذج

الأمر بسيط جداً، وقد صممت الكود ليكون سهل الاستخدام والتجربة.

#### 1. تثبيت المكتبات المطلوبة:

كل ما تحتاجه هو `numpy` و `scikit-learn`. يمكنك تثبيتها بالأمر التالي:

```bash
pip install numpy scikit-learn
```

#### 2. تشغيل التدريب:

لتشغيل عملية التدريب، فقط قم بتنفيذ الملف الرئيسي:

```bash
python train.py
```

#### 3. **تجربة النماذج المختلفة (الجزء الممتع!):**

لتسهيل المقارنة، أضفت "لوحة تحكم" بسيطة في أعلى ملف `train.py`. يمكنك من خلالها التبديل بين النموذج الأساسي والنموذج المحسن.

- **لتشغيل النموذج الأساسي (SGD):**

  ```python
  CHOSEN_OPTIMIZER = 'sgd'
  USE_HE_INIT = False
  L2_PENALTY = 0
  ```

- **لتشغيل النموذج المحسن (Adam + He + L2):**
  ```python
  CHOSEN_OPTIMIZER = 'adam'
  USE_HE_INIT = True
  L2_PENALTY = 1e-4  # قيمة صغيرة لمنع الإفراط في التعلم
  ```

---

## رحلة التطوير والنتائج التي توصلت إليها

بدأت ببناء نموذج أساسي باستخدام الممارسات القياسية: مُحسِّن `SGD` بسيط وتهيئة أوزان عادية. هذا النموذج عمل بشكل جيد وأعطى دقة ممتازة بلغت حوالي **97.1%** على مجموعة اختبار MNIST.

بعد ذلك، قررت أن أطور النموذج وأرى ما إذا كان بإمكاني تحسين الأداء. قمت بتطبيق ثلاث تقنيات متقدمة:

1.  **مُحسِّن Adam:** كبديل أذكى لـ `SGD`.
2.  **تهيئة He:** وهي طريقة أفضل لتهيئة الأوزان الأولية للشبكات التي تستخدم `ReLU`.
3.  **تنظيم L2 (Weight Decay):** للمساعدة في منع "الإفراط في التعلم".

عندما قارنت النتائج، لاحظت فرقين رئيسيين:

- **سرعة تعلم أعلى:** النموذج الذي يستخدم `Adam` وصل إلى مستويات دقة عالية (مثل 95%) **بشكل أسرع بكثير** من نموذج `SGD`.
- **أداء نهائي أفضل قليلاً:** الدقة النهائية على بيانات الاختبار ارتفعت بشكل طفيف إلى حوالي **97.3%**، مما يثبت أن هذه التحسينات لها تأثير عملي.

كانت هذه التجربة مفيدة جداً، حيث أظهرت لي أن هذه التقنيات المتقدمة ليست مجرد مفاهيم نظرية، بل لها تأثير حقيقي وملموس على كفاءة وأداء الشبكات العصبونية.
