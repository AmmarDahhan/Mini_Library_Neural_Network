====================================================================================================
PROJECT EXPORT FOR LLMs
====================================================================================================

PROJECT INFORMATION:
--------------------------------------------------
Project Name: HW_Mini_Library_Neural_Network
Generated On: 2026-01-01 20:53:10 (Asia/Damascus / GMT+03:00)
Total Files Processed: 8
Export Tool: Easy Whole Project to Single Text File for LLMs v1.1.0
Tool Author: Jota / JosÃ© Guilherme Pandolfi

EXPORT CONFIGURATION:
--------------------------------------------------
Language: en
Max File Size: 1 MB
Include Hidden Files: false
Output Format: both
Notification Level: minimal
Custom File Name Pattern: {workspaceName}-output

================================================================================
PROJECT STRUCTURE
================================================================================
â”œâ”€â”€ ğŸ“ __pycache__/
â”‚   â”œâ”€â”€ ğŸ“„ layers.cpython-312.pyc (5.04 KB)
â”‚   â”œâ”€â”€ ğŸ“„ network.cpython-312.pyc (4.92 KB)
â”‚   â””â”€â”€ ğŸ“„ optimizer.cpython-312.pyc (2.76 KB)
â”œâ”€â”€ ğŸ“„ layers.py (2.82 KB)
â”œâ”€â”€ ğŸ“„ network.py (3.03 KB)
â”œâ”€â”€ ğŸ“„ optimizer.py (1.34 KB)
â”œâ”€â”€ ğŸ“„ README.md (5.68 KB)
â””â”€â”€ ğŸ“„ train.py (2.36 KB)

================================================================================
PROJECT STATISTICS
================================================================================
Total Files: 8
Total Directories: 1
Text Files: 5
Binary Files: 3
Total Size: 27.95 KB

FILE TYPES DISTRIBUTION:
------------------------------
.py             : 4
.pyc            : 3
.md             : 1

================================================================================
FILE CODE CONTENTS
================================================================================


================================================================================
BINARY/EXCLUDED FILES (not included in text content)
================================================================================
- __pycache__/layers.cpython-312.pyc
- __pycache__/network.cpython-312.pyc
- __pycache__/optimizer.cpython-312.pyc


================================================================================
FILE: layers.py
================================================================================

FILE INFORMATION:
----------------------------------------
Size: 2.82 KB
Extension: .py
Language: python
Location: layers.py
Relative Path: root
Created: 2025-12-27 18:20:49 (Asia/Damascus / GMT+03:00)
Modified: 2026-01-01 20:34:28 (Asia/Damascus / GMT+03:00)
MD5: 3407f2dd239767b74d8511f868cc534c
SHA256: 354c304efdf98f8e28d1ac9e5431b0d1675c5eb6ce36d23a99346597d228252e
Encoding: ASCII

FILE CONTENT:
----------------------------------------
import numpy as np

# Helper Func.

def softmax(input_data):
    stabilized_data = input_data - np.max(input_data,axis=1, keepdims=True)
    exponentails = np.exp(stabilized_data)

    return exponentails / np.sum(exponentails,axis=-1,keepdims=True) 


def cross_entropy_error(predicted_probs,true_labels):
    if predicted_probs.ndim == 1:
        predicted_probs = predicted_probs.reshape(1,predicted_probs.size)
        true_labels = true_labels.reshape(1,true_labels.size)

    if true_labels.ndim== 2:
        true_labels = true_labels.argmax(axis=1)
    batch_size = predicted_probs.shape[0]
    correct_log_probs = np.log(predicted_probs[np.arange(batch_size),true_labels]+ 1e-7)
    loos = -np.sum(correct_log_probs)/batch_size

    return loos

# Layers

class Relu:
    def __init__(self):
        self.zero_mask = None

    def forward(self,input_data):
        self.zero_mask = (input_data <=0)
        output_data = input_data.copy()
        output_data[self.zero_mask]= 0

        return output_data
    
    def backward(self,grad_from_next_layer):
        grad_from_next_layer[self.zero_mask]= 0
        grad_for_prev_layer= grad_from_next_layer

        return grad_for_prev_layer
    
class Sigmoid:
    def __init__(self):
        self.output= None

    def forward(self,input_data):
        result= 1 /(1+ np.exp(-input_data))
        self.output = result
        return result
    
    def backward(self, grad_from_next_layer):
        grade_for_prev_layer= grad_from_next_layer * (1.0 - self.output) * self.output
        return grade_for_prev_layer
    
class Affine:
    def __init__(self,weights,bias):
        self.W= weights
        self.b= bias

        self.input_data = None
        self.grad_w= None
        self.grad_b= None

    def forward(self,input_data):
        self.input_data= input_data
        result = np.dot(input_data,self.W) +self.b
        
        return result
    
    def backward(self,grad_from_next_layer):
        grad_for_prev_layer= np.dot(grad_from_next_layer,self.W.T)
        self.grad_w = np.dot(self.input_data.T,grad_from_next_layer)
        self.grad_b= np.sum(grad_from_next_layer,axis=0)

        return grad_for_prev_layer
    
class SoftMaxWithLoss:
    def __init__(self):
        self.loss = None
        self.predicted_probs= None
        self.true_labels  = None
    
    def forward(self,input_data,true_labels):
        self.true_labels= true_labels
        self.predicted_probs= softmax(input_data)
        self.loss= cross_entropy_error(self.predicted_probs,self.true_labels)

        return self.loss
    
    def backward(self,dout= 1):
        batch_size= self.true_labels.shape[0]
        grad_for_prev_layer= (self.predicted_probs - self.true_labels)/ batch_size

        return grad_for_prev_layer
    
    

================================================================================


================================================================================
FILE: network.py
================================================================================

FILE INFORMATION:
----------------------------------------
Size: 3.03 KB
Extension: .py
Language: python
Location: network.py
Relative Path: root
Created: 2025-12-27 18:20:58 (Asia/Damascus / GMT+03:00)
Modified: 2026-01-01 20:33:34 (Asia/Damascus / GMT+03:00)
MD5: dea3876424825d84a4c63602e2a41a51
SHA256: 5fab9eec94f220119b1bba20de90b9303224a957a803f8b9bb8e22993eb90b9b
Encoding: ASCII

FILE CONTENT:
----------------------------------------
import numpy as np
from collections import OrderedDict
from layers import Affine,Relu,SoftMaxWithLoss

class TwoLayerNet:
    def __init__(self,input_size,hidden_size,output_size,weight_init_type= 'std',l2_penalty= 0):
        self.network_params = {}
        init_type= weight_init_type.lower()

        if init_type== 'he':
            scale1= np.sqrt(2.0/ input_size)
            scale2= np.sqrt(2.0/ hidden_size)
        else:
            scale1= 0.01
            scale2= 0.01

        self.network_params['W1']= scale1 * np.random.randn(input_size,hidden_size)
        self.network_params['b1']= np.zeros(hidden_size)
        self.network_params['W2']= scale2 * np.random.randn(hidden_size,output_size)
        self.network_params['b2']= np.zeros(output_size)

        self.layer_stack= OrderedDict()
        self.layer_stack['Affine1']= Affine(self.network_params['W1'],self.network_params['b1'])
        self.layer_stack['Relu1']= Relu()
        self.layer_stack['Affine2']= Affine(self.network_params['W2'],self.network_params['b2'])

        self.final_loss_layer= SoftMaxWithLoss()
        self.l2_penalty_strength= l2_penalty

    def predict(self,input_data):
        current_data= input_data
        for layer in self.layer_stack.values():
            current_data= layer.forward(current_data)
            
        return current_data
        
    def calculate_loss(self,input_data,target_labels):
        prediction_output = self.predict(input_data)
        l2_penalty_value= 0

        if self.l2_penalty_strength >0:
            w1_penalty= 0.5 * self.l2_penalty_strength * np.sum(self.network_params['W1']**2)
            w2_penalty= 0.5 * self.l2_penalty_strength * np.sum(self.network_params['W2']**2)
            l2_penalty_value= w1_penalty+ w2_penalty

        return self.final_loss_layer.forward(prediction_output,target_labels) + l2_penalty_value
    
    def calculate_accuracy(self,input_data,target_labels):
        predicted_scores= self.predict(input_data)
        predicted_classes= np.argmax(predicted_scores,axis=1)

        if target_labels.ndim !=1:
            target_labels =  np.argmax(target_labels,axis=1)

        accuracy= np.sum(predicted_classes== target_labels)/ float(input_data.shape[0])

        return accuracy
    
    def calculate_gradients(self,input_data,target_labels):
        self.calculate_loss(input_data,target_labels)
        current_grad= self.final_loss_layer.backward(1)
        layers= list(self.layer_stack.values())
        layers.reverse()

        for layer in layers:
            current_grad= layer.backward(current_grad)

        final_gradients= {}
        final_gradients['W1']= self.layer_stack['Affine1'].grad_w + self.l2_penalty_strength* self.network_params['W1']
        final_gradients['b1']= self.layer_stack['Affine1'].grad_b
        final_gradients['W2']= self.layer_stack['Affine2'].grad_w + self.l2_penalty_strength* self.network_params['W2']
        final_gradients['b2']= self.layer_stack['Affine2'].grad_b

        return final_gradients

================================================================================


================================================================================
FILE: optimizer.py
================================================================================

FILE INFORMATION:
----------------------------------------
Size: 1.34 KB
Extension: .py
Language: python
Location: optimizer.py
Relative Path: root
Created: 2025-12-27 18:21:05 (Asia/Damascus / GMT+03:00)
Modified: 2026-01-01 20:36:22 (Asia/Damascus / GMT+03:00)
MD5: 6fa0116de42c92a1f1e955b3f3fb1183
SHA256: 80ddf008068abbe922f275d371de83f47917c9c5f2fc91c772aca9396621f203
Encoding: ASCII

FILE CONTENT:
----------------------------------------
import numpy as np 

class SGD:
    def __init__(self,learning_rate=0.01):
        self.lr = learning_rate

    def apply_updates(self,network_parameters,calculate_gradients):
        for key in network_parameters.keys():
            network_parameters[key]-= self.lr*calculate_gradients[key]

class Adam:
    def __init__(self,learning_rate=0.001,beta1=0.9,beta2=0.999):
        self.lr= learning_rate
        self.beta1= beta1
        self.beta2= beta2
        self.step= 0
        self.m= None
        self.v= None

    def apply_updates(self,network_parameters,calculate_gradients):
        if self.m is None:
            self.m,self.v= {},{}
            for key,val in network_parameters.items():
                self.m[key]= np.zeros_like(val)
                self.v[key]= np.zeros_like(val)

        self.step+= 1

        for key in network_parameters.keys():
            grad= calculate_gradients[key]
            self.m[key]= self.beta1*self.m[key] + (1- self.beta1)*grad
            self.v[key]= self.beta2*self.v[key] + (1- self.beta2)*(grad**2)
            m_corrected= self.m[key]/ (1- self.beta1**self.step)
            v_corrected= self.v[key]/ (1- self.beta2**self.step)

            update_value= self.lr*m_corrected/ (np.sqrt(v_corrected)+ 1e-7)
            network_parameters[key]-= update_value
            
        
================================================================================


================================================================================
FILE: README.md
================================================================================

FILE INFORMATION:
----------------------------------------
Size: 5.68 KB
Extension: .md
Language: text
Location: README.md
Relative Path: root
Created: 2026-01-01 20:51:23 (Asia/Damascus / GMT+03:00)
Modified: 2026-01-01 20:53:10 (Asia/Damascus / GMT+03:00)
MD5: 660813d404e4399bcee748fd3657e1ee
SHA256: bb206e99d3afda6327e236b90993624ee85483469dec59061ff181744bc6f071
Encoding: UTF-8

FILE CONTENT:
----------------------------------------
# Ø¨Ù†Ø§Ø¡ Ù…ÙƒØªØ¨Ø© Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ù…ØµØºØ±Ø© Ù…Ù† Ø§Ù„ØµÙØ±

## Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù‡Ùˆ ØªØ·Ø¨ÙŠÙ‚ÙŠ Ù„ÙˆØ¸ÙŠÙØ© Ø¬Ø§Ù…Ø¹ÙŠØ© ØªÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ù…ÙƒØªØ¨Ø© Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙˆÙ†ÙŠØ© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„ØµÙØ± Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¹Ù„Ù‰ Ù…ÙƒØªØ¨Ø© `NumPy`. Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙƒØ§Ù† ÙÙ‡Ù… Ø§Ù„Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø¬ÙˆÙ‡Ø±ÙŠØ© Ù„Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙˆÙ†ÙŠØ©ØŒ Ù…Ø«Ù„ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ ÙˆØ§Ù„Ø®Ù„ÙÙŠØŒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¨Ø±Ù…Ø¬ØªÙ‡Ø§ ÙŠØ¯ÙˆÙŠØ§Ù‹ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø·Ø± Ø¹Ù…Ù„ Ø¬Ø§Ù‡Ø²Ø© Ù…Ø«Ù„ PyTorch Ø£Ùˆ TensorFlow.

---

## Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ… ØªØ·Ø¨ÙŠÙ‚Ù‡Ø§

- **Ø§Ù„Ø·Ø¨Ù‚Ø§Øª (Layers):** Ù‚Ù…Øª Ø¨Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
  - `Affine`: Ø§Ù„Ø·Ø¨Ù‚Ø© Ù…ØªØµÙ„Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (Fully Connected).
  - `ReLU`: Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø´ÙŠØ·.
  - `SoftmaxWithLoss`: Ø·Ø¨Ù‚Ø© Ù…Ø¯Ù…Ø¬Ø© Ù„Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·Ø£.
- **Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ (Forward Propagation):** ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø´Ø¨ÙƒØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª.
- **Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ (Backpropagation):** Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª (Gradients) Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙˆØ§Ù„Ø§Ù†Ø­ÙŠØ§Ø²Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø³Ù„Ø³Ù„Ø©.
- **Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†Ø§Øª (Optimizers):** Ù‚Ù…Øª Ø¨ØªØ·Ø¨ÙŠÙ‚ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØªÙŠÙ† Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†:
  - `SGD`: Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·.
  - `Adam`: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø£ÙƒØ«Ø± ØªØ·ÙˆØ±Ø§Ù‹ ÙˆÙØ¹Ø§Ù„ÙŠØ©.
- **Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Training Loop):** Ø³ÙƒØ±Ø¨Øª ÙƒØ§Ù…Ù„ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª MNISTØŒ Ù…Ø¹ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø§Øª (mini-batches) ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©.

---

## Ù‡ÙŠÙƒÙ„ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù…Ù‚Ø³Ù… Ø¥Ù„Ù‰ 4 Ù…Ù„ÙØ§Øª Ø±Ø¦ÙŠØ³ÙŠØ©ØŒ ÙƒÙ„ Ù…Ù†Ù‡Ø§ Ù„Ù‡ ÙˆØ¸ÙŠÙØ© Ù…Ø­Ø¯Ø¯Ø©:

- **`layers.py`**: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "Ù‚Ø·Ø¹ Ø§Ù„Ù„ÙŠØºÙˆ" Ù„Ù„Ù…Ø´Ø±ÙˆØ¹. ÙƒÙ„ ÙƒÙ„Ø§Ø³ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠÙ…Ø«Ù„ Ø·Ø¨Ù‚Ø© (Ù…Ø«Ù„ `Relu` Ø£Ùˆ `Affine`) ÙˆÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¯Ø§Ù„ØªÙŠ `forward` Ùˆ `backward` Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØªÙŠÙ†.

- **`network.py`**: Ù‡Ù†Ø§ Ù‚Ù…Øª Ø¨ØªØ¹Ø±ÙŠÙ ÙƒÙ„Ø§Ø³ `TwoLayerNet` Ø§Ù„Ø°ÙŠ ÙŠØ¬Ù…Ø¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ù…Ù† `layers.py` Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙˆÙ†ÙŠØ©.

- **`optimizer.py`**: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "Ù…Ø­Ø±ÙƒØ§Øª" Ø§Ù„ØªØ¹Ù„Ù…. Ù‚Ù…Øª Ø¨ØªØ·Ø¨ÙŠÙ‚ `SGD` Ùˆ `Adam` Ù‡Ù†Ø§ØŒ ÙˆÙ‡ÙŠ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† ØªØ­Ø¯ÙŠØ« Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø´Ø¨ÙƒØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø­Ø³ÙˆØ¨Ø©.

- **`train.py`**: Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ø°ÙŠ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ´ØºÙŠÙ„ ÙƒÙ„ Ø´ÙŠØ¡. ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª MNISTØŒ ÙˆØ¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ù…Ù† Ø§Ù„Ø´Ø¨ÙƒØ© ÙˆØ§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†ØŒ Ø«Ù… ÙŠØ¨Ø¯Ø£ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ….

---

## ÙƒÙŠÙÙŠØ© ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬

Ø§Ù„Ø£Ù…Ø± Ø¨Ø³ÙŠØ· Ø¬Ø¯Ø§Ù‹ØŒ ÙˆÙ‚Ø¯ ØµÙ…Ù…Øª Ø§Ù„ÙƒÙˆØ¯ Ù„ÙŠÙƒÙˆÙ† Ø³Ù‡Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ù„ØªØ¬Ø±Ø¨Ø©.

#### 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:

ÙƒÙ„ Ù…Ø§ ØªØ­ØªØ§Ø¬Ù‡ Ù‡Ùˆ `numpy` Ùˆ `scikit-learn`. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ«Ø¨ÙŠØªÙ‡Ø§ Ø¨Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠ:

```bash
pip install numpy scikit-learn
```

#### 2. ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:

Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙÙ‚Ø· Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ:

```bash
python train.py
```

#### 3. **ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© (Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù…ØªØ¹!):**

Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©ØŒ Ø£Ø¶ÙØª "Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ…" Ø¨Ø³ÙŠØ·Ø© ÙÙŠ Ø£Ø¹Ù„Ù‰ Ù…Ù„Ù `train.py`. ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„Ù‡Ø§ Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†.

- **Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (SGD):**

  ```python
  CHOSEN_OPTIMIZER = 'sgd'
  USE_HE_INIT = False
  L2_PENALTY = 0
  ```

- **Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù† (Adam + He + L2):**
  ```python
  CHOSEN_OPTIMIZER = 'adam'
  USE_HE_INIT = True
  L2_PENALTY = 1e-4  # Ù‚ÙŠÙ…Ø© ØµØºÙŠØ±Ø© Ù„Ù…Ù†Ø¹ Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù…
  ```

---

## Ø±Ø­Ù„Ø© Ø§Ù„ØªØ·ÙˆÙŠØ± ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙŠ ØªÙˆØµÙ„Øª Ø¥Ù„ÙŠÙ‡Ø§

Ø¨Ø¯Ø£Øª Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ©: Ù…ÙØ­Ø³ÙÙ‘Ù† `SGD` Ø¨Ø³ÙŠØ· ÙˆØªÙ‡ÙŠØ¦Ø© Ø£ÙˆØ²Ø§Ù† Ø¹Ø§Ø¯ÙŠØ©. Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ ÙˆØ£Ø¹Ø·Ù‰ Ø¯Ù‚Ø© Ù…Ù…ØªØ§Ø²Ø© Ø¨Ù„ØºØª Ø­ÙˆØ§Ù„ÙŠ **97.1%** Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø± MNIST.

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ø±Ø±Øª Ø£Ù† Ø£Ø·ÙˆØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ£Ø±Ù‰ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¨Ø¥Ù…ÙƒØ§Ù†ÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡. Ù‚Ù…Øª Ø¨ØªØ·Ø¨ÙŠÙ‚ Ø«Ù„Ø§Ø« ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©:

1.  **Ù…ÙØ­Ø³ÙÙ‘Ù† Adam:** ÙƒØ¨Ø¯ÙŠÙ„ Ø£Ø°ÙƒÙ‰ Ù„Ù€ `SGD`.
2.  **ØªÙ‡ÙŠØ¦Ø© He:** ÙˆÙ‡ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø£ÙØ¶Ù„ Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù„Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… `ReLU`.
3.  **ØªÙ†Ø¸ÙŠÙ… L2 (Weight Decay):** Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ù…Ù†Ø¹ "Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù…".

Ø¹Ù†Ø¯Ù…Ø§ Ù‚Ø§Ø±Ù†Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ù„Ø§Ø­Ø¸Øª ÙØ±Ù‚ÙŠÙ† Ø±Ø¦ÙŠØ³ÙŠÙŠÙ†:

- **Ø³Ø±Ø¹Ø© ØªØ¹Ù„Ù… Ø£Ø¹Ù„Ù‰:** Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ÙŠØ³ØªØ®Ø¯Ù… `Adam` ÙˆØµÙ„ Ø¥Ù„Ù‰ Ù…Ø³ØªÙˆÙŠØ§Øª Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (Ù…Ø«Ù„ 95%) **Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø±Ø¹ Ø¨ÙƒØ«ÙŠØ±** Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ `SGD`.
- **Ø£Ø¯Ø§Ø¡ Ù†Ù‡Ø§Ø¦ÙŠ Ø£ÙØ¶Ù„ Ù‚Ù„ÙŠÙ„Ø§Ù‹:** Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø±ØªÙØ¹Øª Ø¨Ø´ÙƒÙ„ Ø·ÙÙŠÙ Ø¥Ù„Ù‰ Ø­ÙˆØ§Ù„ÙŠ **97.3%**ØŒ Ù…Ù…Ø§ ÙŠØ«Ø¨Øª Ø£Ù† Ù‡Ø°Ù‡ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ù„Ù‡Ø§ ØªØ£Ø«ÙŠØ± Ø¹Ù…Ù„ÙŠ.

ÙƒØ§Ù†Øª Ù‡Ø°Ù‡ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ù…ÙÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ø­ÙŠØ« Ø£Ø¸Ù‡Ø±Øª Ù„ÙŠ Ø£Ù† Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ÙŠØ³Øª Ù…Ø¬Ø±Ø¯ Ù…ÙØ§Ù‡ÙŠÙ… Ù†Ø¸Ø±ÙŠØ©ØŒ Ø¨Ù„ Ù„Ù‡Ø§ ØªØ£Ø«ÙŠØ± Ø­Ù‚ÙŠÙ‚ÙŠ ÙˆÙ…Ù„Ù…ÙˆØ³ Ø¹Ù„Ù‰ ÙƒÙØ§Ø¡Ø© ÙˆØ£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙˆÙ†ÙŠØ©.

================================================================================


================================================================================
FILE: train.py
================================================================================

FILE INFORMATION:
----------------------------------------
Size: 2.36 KB
Extension: .py
Language: python
Location: train.py
Relative Path: root
Created: 2025-12-27 18:21:11 (Asia/Damascus / GMT+03:00)
Modified: 2026-01-01 20:39:18 (Asia/Damascus / GMT+03:00)
MD5: 0415dfd7e86d10c1440e5aa925ebf66b
SHA256: 30ac644da9989c804dc522b8d0e57b74c52829fa51dac2481c5023f6f0cab3c6
Encoding: ASCII

FILE CONTENT:
----------------------------------------
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import OneHotEncoder
from network import TwoLayerNet
from optimizer import SGD, Adam

CHOSEN_OPTIMIZER = 'sgd'     # sgd or adam
USE_HE_INIT = False            # True or False
L2_PENALTY = 0             # 0 or 1e-4


np.random.seed(42) 


print("Data loading MNIST " )
X_data,y_data= fetch_openml('mnist_784',version=1,return_X_y=True,as_frame=False)
X_data= X_data/ 255.0
y_data_labels= y_data.astype(np.uint8)
encoder= OneHotEncoder(sparse_output=False)
y_data_onehot= encoder.fit_transform(y_data_labels.reshape(-1, 1))
X_train,X_test= X_data[:60000],X_data[60000:]
y_train, y_test= y_data_onehot[:60000],y_data_onehot[60000:]
y_train_labels,y_test_labels= y_data_labels[:60000],y_data_labels[60000:]

print("The data was uploaded successfully")


total_training_steps=10000
training_data_size =X_train.shape[0]
batch_size= 100

print("\nNetwork settings")
print(f"engine (Optimizer): {CHOSEN_OPTIMIZER}")
print(f"Using the configuration He: {USE_HE_INIT}")
print(f"Penalty L2: {L2_PENALTY}")
print("\n")

network= TwoLayerNet(input_size=784,hidden_size=50,output_size=10,weight_init_type='he' if USE_HE_INIT else 'std',l2_penalty=L2_PENALTY)

if CHOSEN_OPTIMIZER.lower() == 'adam':
    optimizer= Adam(learning_rate=0.001)
else:
    optimizer= SGD(learning_rate=0.1)

print("Start of training session\n")

for step in range(total_training_steps):
    random_indices= np.random.choice(training_data_size,batch_size)
    input_batch= X_train[random_indices]
    target_batch= y_train[random_indices]   
    gradients= network.calculate_gradients(input_batch,target_batch)
    optimizer.apply_updates(network.network_params,gradients)
    
    if step % 100 == 0:
        current_loss= network.calculate_loss(input_batch,target_batch)
        accuracy_on_train= network.calculate_accuracy(X_train,y_train_labels)
        accuracy_on_test= network.calculate_accuracy(X_test,y_test_labels)
        
        print(f"the step {step}/ {total_training_steps} | the error: {current_loss:.4f} | Training accuracy : {accuracy_on_train:.4f} | Test accuracy: {accuracy_on_test:.4f}")

print("\n Training End" )

final_accuracy= network.calculate_accuracy(X_test,y_test_labels)
print("=" * 40)
print(f"Final accuracy on test data: {final_accuracy:.4f}")
print("=" * 40)

================================================================================
